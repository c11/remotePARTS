---
title: "Remote-sensing-code_notes"
author: "Clay Morrow"
date: "7/6/2020"
output: 
  html_document:
    highlight: tango
    theme: "darkly"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = NA)
knitr::opts_knit$get()
```

```{r packages}
# library(dplyr);library(data.table)
```


This document is meant to record the notes for the remote-sensing R code to be
used in the remoteSTAR package. This includes Tony's original code and any 
additional code that I add. 

All file paths used in this document will be discussed relative to the main 
`remoteSTAR` project folder on Box.com: 
`"Box/NASA statistics project/NASA Morrow folder/r-package_code/remoteSTAR/"`

# Formatting Preferences

Here I will keep track of my code formatting preferences to but used in the 
`remoteSTAR` package

* Each line should contain no more than 80 characters wherever possible. This
applies to both `.R` and `.Rmd` files. This is to keep files readible and to
prevent the need for side-scrolling when reading through code. Though, when
referencing original files, I will keep the text as-written. 
  + **Comments** are allowed to go beyond the 80chr line when they are on the 
  same line as code. These comments to the right of code, however, 
  should be very short.
* In general, I will try to follow conventions outlined in the [R style guide](https://style.tidyverse.org/files.html#names)

# Things to focus on

* optimizing the code for speed and efficiency (particularly when it comes to
matrix operations)

* optimize the code in terms of concision - seperate functions, etc. 

* use classes and methods instead of just returning a bunch of stuff. 

# Tony's original code files

First, I will go over the original code to take notes on what each relevant 
piece does and how it has been integrated into the `remoteSTAR` package. 
Tony's original code can be found in the `../original-example-code_from-tony`
directory.

## Example of remote-sensing analysis {#examples}

The `"../original-example-code_from-tony/example code for NDVI North America.R"`
file contains an example analysis using a WI subset of the NDVI data. 

First, packages for remote sensing and matrix calculus are loaded and the R 
script containing the main functions `remote_sensing_tools_24Mar20.R` (I'll 
address these functions as they come up) is loaded

### Isolate Wisconsin 

```{r, eval = FALSE}
library(Matrix)
library(LatticeKrig)
library(geosphere)
library(colorspace)

source('remote_sensing_tools_24Mar20.R')
```

and the WI NDVI data, at 32 time points, is isolated.

```{r, eval = FALSE}
data <- read.csv(file="north_america_checked.csv")
summary(data)

# to create a play dataset, isolate WI
wi.data <- data[(data$lng > -92) & (data$lng < -87) & (data$lat > 42) & (data$lat < 47),]
write.table(file="wisconsin_checked.csv", wi.data, sep=",", row.names=F)

data <- read.csv(file="wisconsin_checked.csv")
summary(data)

sort(unique(data$land))
n.obs <- 32
t.scale <- 1:n.obs
t.scale <- (t.scale-min(t.scale))/max(t.scale)
```

The land classes which are then renamed and rare land
classes are removed. 

```{r, eval = FALSE}
# Do some basic data construction to sort landcover classes

# landclasses: 1 Evergreen needleleaf forests 2 evergreen broadleaf forests, 3 deciduous needleleaf forests, 4 deciduous broadleaf forests, 5 mixed forests, 6 shrublands, 8 savannas, 10 grasslands, 12 croplands, 14 croplands/natural vegetation mosaics.

landclasses <- c("Evergr needle","Evergr broad","Decid needle","Decid broad","Mixed forest","Shrubland","Savanna","Grassland","Cropland","Cropland mosaics")
data$landclass <- landclasses[1]
count <- 0
for(i in sort(unique(data$land))) {
	count <- count+1
	data$landclass[data$land==i] <- landclasses[count]
}
data$landclass <- as.factor(data$landclass)
n.classes <- aggregate(data$landclass, by=list(landscape=data$landclass), FUN=length)
rare.class.threshold <- 0.005 * nrow(data)
rare.classes <- n.classes$landscape[n.classes$x <= rare.class.threshold]
data <- data[!is.element(data$landclass, rare.classes),]
data$landclass <- droplevels(data$landclass)
levels(data$landclass)

landclasses <- c("Evergr needle","Evergr broad","Decid broad","Mixed forest","Shrubland","Savanna","Grassland","Cropland")

data$landclass.num <- 1
for(i in 1:length(landclasses)) data$landclass.num[data$landclass == landclasses[i]] <- i

n.classes <- aggregate(data$landclass, by=list(landscape=data$landclass), FUN=length)
n.classes
      # landscape    x
# 1  Decid needle  755
# 2  Evergr broad   78
# 3 Evergr needle  781
# 4  Mixed forest 1055
# 5     Shrubland   51
```

**NOTE:** the newer version of this file analyzes Alaska instead of WI. It 
includes additional code to check for and remove outliers, among other things.

### Analyze

The data are then plotted with colors according to their land class.

```{r, eval = FALSE}
# plot map of data
minlng <- min(data$lng)
maxlng <- max(data$lng)

col.pal <- terrain.colors(nlevels(data$landclass))
plot(lat ~ lng, data=data, pch=20, cex=.1, col=col.pal[landclass.num], xlim=c(minlng, maxlng), xlab="", ylab="")
legend(x=-180, y=45, legend=landclasses, col=col.pal, pch=20, bty="n")
```

```{r, echo = FALSE}
knitr::include_graphics("../../original-example-code_from-tony/WI_lat-long-landclass.jpeg")
```

Then constrained least squares (CLS) analysis is performed using the `CLS.fit()`
function and the spatial correlation is fit to a subset of 2K random points 
using `spatialcor.fit.data()` (both functions discussed in 
[Remote sensing functions](#functions) section).

```{r, eval = FALSE}
# perform CLS

# create distance matrix in kilometers
location <- data[,c('lng','lat')]
Dist <- distm(location, fun=distGeo)/1000

# fit CLS
X <- as.matrix(data[,6:37])
dat.map <- CLS.fit(X, t.scale)
n <- nrow(dat.map)

# this adds the landscape variable to dat.map
dat.map$landclass <- data$landclass
dat.map$landclass <- droplevels(dat.map$landclass)
dat.map$lat <- data$lat
dat.map$lng <- data$lng
dat.map$c.cls <- dat.map$c
dat.map$c <- dat.map$c.cls/(1-dat.map$b)
dat.map$rel.c.cls <- dat.map$c.cls/dat.map$mean
dat.map$abslat <- abs(dat.map$lat)

# fit the spatial correlation for a random subset of fit.n.sample points
fit.n.sample <- 2000
r.est <- spatialcor.fit.data(X, t.scale, data=data, fit.n.sample=fit.n.sample, r.start=.1, plot.fig=T)
```

A generalized least squares (GLS) version is also fit

```{r, eval = FALSE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# perform the full analysis using GLS.fit
# NOTE: GLS.fit is a base-level function that requires fitting the nugget separately and constructing the covariance matrix
V <- V.fit(Dist, spatialcor=r.est$spatialcor, FUN="exponential")

# construct the GLS correlation matrix and fit the GLS
# absolute c.cls
nugget <- nugget.fit(formula='rel.c.cls ~ 0 + landclass', dat.map, V, nugget.tol = 0.00001, verbose = T)
Vn <- (1 - nugget) * V + nugget * diag(n)
nugget

invcholVn <- t(backsolve(chol(Vn), diag(n)))
z.GLS.fit <- GLS.fit(rel.c.cls ~ 0 + landclass, data=dat.map, invcholV=invcholVn)
names(z.GLS.fit)
as.data.frame(z.GLS.fit[c("coef","se","t","df.t","p.t")])
as.data.frame(z.GLS.fit[c("F","df1.F","df2.F","p.F","logLik","logLik0")])
```

as well as a partitioned variant

```{r, eval = FALSE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# perform the analysis by partitioning data using GLS.partition.data

# number of partitions is npart
npart <- 4
z.GLS.partition <- GLS.partition.data(rel.c.cls ~ 0 + landclass, formula0=rel.c.cls ~ 1, data=dat.map, r=r.est$spatialcor, est.nugget=T, npart=npart)
names(z.GLS.partition)
z.pvalue <- GLS.partition.pvalue(z.GLS.partition, nboot = 10^5)
as.data.frame(z.GLS.partition[c("coef","Fmean", "df1", "df2")])
as.data.frame(z.GLS.partition[c("F.part", "p.F.part")])
z.pvalue
```


## Remote sensing functions {#functions}

This section describes what the relevant (used in the 
[above example](#examples)) remote sensing functions 

### `CLS.fit(X, t.scale)`

Regresses the current NDVI score `X` on the previous time's NDVI score, conditioned
upon the current scaled time variable `t.scale`. 

The function fits the model regression model $Y_t = Y_{t-1} + T_t$ where $Y_{t}$
is the NDVI at time $t$ and $T_t$ is the constrained, ordered, time variable. 
Here $0 \leq T_{t} \leq 1$ for all $t$. It then extracts the relevant information
from the coefficient table. 


```{r, eval = FALSE}
############################################################################
# CLS.fit fits a CLS model using lm() to a matrix of time series located in each row
############################################################################
CLS.fit <- function(X, t.scale) {

	n <- dim(X)[1]

	# CLS for entire map
	d <- data.frame(site = 1:n)
	for (i in 1:dim(X)[1]) {
		x <- X[i, ]

		d$mean[i] <- mean(x)

		z.CLS <- lm(x[2:length(x)] ~ x[1:(length(x) - 1)] + t.scale[2:length(x)])
		d$c[i] <- summary(z.CLS)$coef[3, 1]
		d$t[i] <- summary(z.CLS)$coef[3, 3]
		d$p[i] <- summary(z.CLS)$coef[3, 4]
		d$b[i] <- summary(z.CLS)$coef[2, 1]
		d$MSE[i] <- summary(z.CLS)$sigma^2
	}
	return(d)
}
```


### `spatialcor.fit.data()`

The `spatialcor.fit.data()` function estimates a non-linear least squares
coefficeint from either an exponential or taper-spherical formula. 

I've added notes to the right of some lines to explain their use.

```{r, eval = FALSE}
############################################################################
# spatialcor.fit.data fits an exponential or taper-spherical distance matrix to a spatial set of time series by performing CLS, calculating the correlation matrix of residuals, and then fitting the correlations using nls. 

# NOTE: In contrast to spatialcor.fit, spatialcor.fit.data returns the spatial correlation scaled to the input distance matrix in km (from distm)
############################################################################
spatialcor.fit.data <- function(X, t.scale, data, r.start = 0.1, fit.n.sample, FUN = "exponential", plot.fig = F, col.plot = NULL) {

	n <- nrow(X) # count the samples (locations, pixels, etc.)

	# subsample for r.fit
	fit.pick <- sample.int(n = n, size = fit.n.sample) # random subset of the sampes

	resid <- matrix(0, nrow = fit.n.sample, ncol = n.obs - 1)
	for (i in 1:fit.n.sample) { # for each subset ... 
 		x <- X[fit.pick[i], ] # look at all times in one location

		z.CLS <- lm(x[2:length(x)] ~ x[1:(length(x) - 1)] + t.scale[2:length(x)]) # fit a CLS model
		resid[i, ] <- z.CLS$resid # extract the residuals
	}
	cor.resid <- cor(t(resid)) # correlation matrix for the residuals
	
	# create distance matrix in kilometers
	location <- data[fit.pick,c('lng','lat')]
	Dist <- geosphere::distm(location, fun=distGeo)/1000	
	dist <- Dist/max(Dist) # distance matrix as a proportion

	# colors for plotting
	if (is.null(col.plot)) {
		col.plot <- "black"
	} else {
		col.plot <- col.plot[fit.pick]
	}

	cor.resid[lower.tri(cor.resid)] <- NA
	dist[lower.tri(dist)] <- NA

	v.cor.resid <- matrix(cor.resid, ncol = 1) # collapse cor matrix into a column vector
	v.dist <- matrix(dist, ncol = 1) # collapse the distance matrix into a column vector
	v.cor.resid <- v.cor.resid[!is.na(v.cor.resid)] # remove nas
	v.dist <- v.dist[!is.na(v.dist)] # remove nas

	w <- as.data.frame(cbind(v.dist, v.cor.resid)) # combine spatial and temporal correlation columns into d!f
	names(w) <- c("dist", "cor")

	if (FUN == "exponential") {
		fit <- nls(cor ~ exp(-dist/r), data = w, start = list(r = r.start))
		spatialcor <- coef(fit) * max(Dist)
	}
	if (FUN == "taper-spherical") { # the ~ is just recognizing a function NOT a model formula
		fit <- nls(~taper.spherical.dif(d = dist, cor = cor, b = b), data = w, start = list(b = 0.5))
		spatialcor <- exp(-coef(fit)) * max(Dist)
	}
	if (plot.fig) {
		plot(dist * max(Dist), cor.resid, pch = 20, cex = 0.5, col = col.plot)
		x.dist <- (1:fit.n.sample)/fit.n.sample * max(Dist)
		if (FUN == "exponential") 
			lines(x.dist, exp(-x.dist/spatialcor), col = "red", lty = 2)
		if (FUN == "taper-spherical") 
			lines(x.dist, taper.spherical(d = x.dist, beta = spatialcor), col = "red", lty = 2)
	}
	return(list(spatialcor = spatialcor, spatialcor.sigma = summary(fit)$sigma))
}
```

if `FUN == "exponential"`, then the exponential version of non-linear least 
squares (nls) model is fit: $s_{i,j} = e^{-d_{i,j}/r}$ where $s_{i,j}$ is the temporal 
correlation between points $i$ and $j$, $d_{i,j}$ is the geographic
distance between points $i$ and $j$, and $r$ is the unknown scaling parameter
to be estimated.

if, instead, `FUN == "taper-spherical"`, $\beta$ is estimated from the formula 
$y = s - x$ where 
$x = \begin{cases} 0 & d < \beta \\ (1 - \frac{d}{\beta})^2 \times (1 + \frac{d}{2\beta}) & else \end{cases}$
as found in the `taper.spherical.diff()` function. 

**However, I don't know what a one-sided formula means in `nls()` what is $y$?**

```{r, eval = FALSE}
taper.spherical.dif <- function(d, cor, b) {
	beta <- exp(-b)
	x <- d
	x[d > beta] <- 0
	x[d <= beta] <- ((1 - d[d <= beta]/beta)^2) * (1 + d[d <= beta]/(2 * beta))
	cor - x
}

taper.spherical <- function(d, beta) {
	x <- d
	x[d > beta] <- 0
	x[d <= beta] <- ((1 - d[d <= beta]/beta)^2) * (1 + d[d <= beta]/(2 * beta))
	x
}
```


### `V.fit()`

This function constructs the covariance matrix from a distance matrix and the
`spatialcor` parameter ($\beta$ or $r$) estimated from `spatial.cor.fit.data()`.

```{r, eval = FALSE}
############################################################################
# V.fit constructs the covariance matrix from a distance matrix for exponential and taper-spherical structures
############################################################################
V.fit <- function(Dist, spatialcor, FUN = "exponential") {

	if (FUN == "exponential") 
		return(exp(-Dist/spatialcor))

	if (FUN == "taper-spherical") 
		return(taper.spherical(Dist, spatialcor))

}
```


### `GLS.fit()`

This function performs a generalized least squares regression based on the
formula provided. The variance/covariance matrix is constructed from the `V`
argument (possibly from the inverse cholesky decomposition of `V`). Typical
regression outputs are returned and an F test of the full model vs. the null
model is also performed. 

notes are added to the right of code. 
```{r, eval = FALSE}
############################################################################
# GLS.fit fits a GLS to the data given a specified V or invcholV
############################################################################
GLS.fit <- function(formula, formula0 = NULL, data, V = NULL, invcholV = NULL) {

	mf <- model.frame(formula = formula, data = data) # extract the model framed
	x <- model.matrix(attr(mf, "terms"), data = mf) # x variables
	y <- model.response(mf) # y variable
	n <- length(y) # number of observations

	if (is.null(invcholV)) { # inverse cholesky decomposition
		if (is.null(V)) {
			invcholV <- diag(n) # identity matrix
		} else {
		  # solve the equation Rx = b (for the upper triangle)
			invcholV <- t(backsolve(chol(V), diag(n))) # invert the chol. decomp. matrix
		}
	}

	xx <- invcholV %*% x # multiply the coefficeint to x
	yy <- invcholV %*% y # and to y
	
	coef <- as.numeric(solve(crossprod(xx), crossprod(xx,yy))) #solve for A in: A(t(xx) %*% xx) = t(xx) %*% yy
	names(coef) <- colnames(x) # rename the coefficients
	varX <- t(xx) %*% xx 
	SSE <- as.numeric(crossprod(yy - xx %*% coef)) # sum of squares error
	MSE <- SSE/(n - ncol(xx)) # mean square error
	
	varcov <- MSE * solve(varX) #MSE * inverse of varX
	se <- diag(varcov)^0.5 # sqrt of diagonal (sd)
	t <- coef/se # t-statistics
	df.t <- n - ncol(xx) # degrees freedom
	p.t <- 2 * pt(abs(t), df = df.t, lower.tail = F) #pvalue

	logdetV <- -2 * sum(log(diag(invcholV))) #??
	logLik <- -0.5 * (n * log(2 * pi) + n * log((n-ncol(xx))*MSE/n) + logdetV + n) 

	# repeat for null model 
	if (is.null(formula0)) { 
		x0 <- matrix(1, nrow = n, ncol = 1)
		xx0 <- invcholV %*% x0
		coef0 <- solve(crossprod(xx0), crossprod(xx0, yy))
		SSE0 <- as.numeric(crossprod(yy - xx0 %*% coef0))
		df0 <- ncol(coef0)
	} else {
		mf0 <- model.frame(formula = formula0, data = data)
		x0 <- model.matrix(attr(mf0, "terms"), data = mf0)
		xx0 <- invcholV %*% x0
		
		if(any(xx0 != 0)){
			coef0 <- solve(crossprod(xx0), crossprod(xx0, yy))
			SSE0 <- as.numeric(crossprod(yy - xx0 %*% coef0))
			df0 <- ncol(coef0)
		}else{
			SSE0 <- as.numeric(crossprod(yy))
			df0 <- 1
			coef0 <- NA
		}
	}
	MSE0 <- SSE0/(n - ncol(xx))
	MSR <- (SSE0 - SSE)/(ncol(xx) - ncol(xx0)) #regression SS
	logLik0 <- -0.5 * (n * log(2 * pi) + n * log((n-df0)*MSE0/n) + logdetV + n)
	
	if(any(xx0 != 0)){	
		varX0 <- t(xx0) %*% xx0	
		varcov0 <- MSE0 * solve(varX0)
		se0 <- diag(varcov0)^0.5
	}else{
		varcov0 <- NULL
		se0 <- NULL
	}
	
  # F test for what?? F = df2/df1 * (SSE0-SSE)/SSE
	if (ncol(xx) > 1) {
		FF <- (n - ncol(xx))/(ncol(xx) - ncol(xx0)) * (SSE0 - SSE)/SSE
		df1.F <- ncol(xx) - ncol(xx0)
		df2.F <- n - ncol(xx)
		p.F <- pf(FF, df1 = df1.F, df2 = df2.F, lower.tail = F)
		df.F <- c(df1.F, df2.F)
	} else {
		FF <- (n - 1) * (SSE0 - SSE)/SSE
		df1.F <- 1
		df2.F <- n - 1
		p.F <- pf(FF, df1 = df1.F, df2 = df2.F, lower.tail = F)
		df.F <- c(df1.F, df2.F)
	}

	return(list(coef = coef, se = se, t = t, df.t = df.t, p.t = p.t, F = FF, df1.F = df1.F, df2.F = df2.F, p.F = p.F, logLik = logLik, logLik0 = logLik0, MSE = MSE, MSE0 = MSE0, MSR = MSR, SSE = SSE, SSE0 = SSE0, SSR = SSE0 - SSE, coef0 = coef0, se0 = se0, varX = varX, varcov = varcov, varcov0 = varcov0, invcholV = invcholV, xx=xx, xx0=xx0, yy=yy))
}
```

#### inverse cholesky variance matrix

Based on the GLS formulae from Ives and Zhu (2006):

- $y=X\beta+ε$

-	$E[\varepsilon\varepsilon^T ]=\sigma^2V$

-	$\hat{\beta} = (X^TV^{-1}X)^{-1}(X^TV^{-1}y)$
  
  +	Which means $\hat{\beta}(X^TV^{-1}X) = (X^TV^{-1}y)$
  
- $\hat\sigma^2 = (y - X \hat{\beta})^{T} V^{-1} (y - X \hat{\beta}) / (n-2)$

And so, one of the main points of the code, from my understanding is to calculate the inverse of V:  V^(-1), which is meant to be done by `t(backsolve(chol(V), diag(n)))` in the `GLS.fit()` function, if I understand correctly. 

+ $V = U^TU$ where the upper triangle of the Cholesky composition is $U = chol(V)$

  - then $V^{-1}=(U^{-1})(U^{-1})^T$

However, this is not what the code does: 

```
AA <- crossprod(matrix(1:6, ncol = 2)) # 2x2 pos-def matrix
cM <- chol(AA) # chol decomp (Upper)
inv.A <- t(backsolve(cM, diag(2))) # invcholV
inv.B <- chol2inv(cM) # inverse of AA from chol decomp
all.equal(inv.A, inv.B) # are they the same??
[1] FALSE
t(inv.A) %*% AA # check inverse (identity expected)
       [,1]   [,2]
[1,] -33.50 -81.06
[2,]  16.29  39.21 
t(inv.A) %*% cM # check if inverse of chol(AA)
     [,1] [,2]
[1,]    1    0
[2,]    0    1
```

Here `Inv.A` is actually the inverse of the Cholesky decomposition ($U^{-1}$)  of AA and not the inverse of AA through the Cholesky decomposition. Is this what the code is supposed to do? If so, I don’t understand why. In the next line of code you do the following: 

```
  xx <- invcholV %*% x
  yy <- invcholV %*% y
```

which is $(U^{-1})^T X$ and $(U^{-1})^T y$ respectively. And then you use 

```
coef <- as.numeric(solve(crossprod(xx), crossprod(xx,yy)))
```

which solves $\hat{\beta} (U^{-1} X^T (U^{-1})^T X) = (U^{-1} X^T (U^{-1})^T y)$. Which seems to be equivalent to the form above from Ives and Zhu. 


### `nugget.fit()`

the function `nugget.fit.funct()` performs `GLS.fit()` using the soultion of 
$Ax = I$ for $A$ as the `invcholV` argument, where $x$ is the choleskey 
decomposition ($LL^*$) of the matrix $L = (1 - n) + n VI$ where
$n$ is the nugget, $V$ is the covariance matrix, and $I$ is an identity matrix.
The function is then optimized to find the best value for $n$.

```{r, eval = FALSE}
############################################################################
# nugget.fit fits a nugget to the covariance matrix V. Note that this involves refitting the data using GLS.fit
############################################################################

nugget.fit.funct <- function(nugget, formula, data, V, verbose = FALSE) {
	n <- ncol(V)
	invcholV <- t(backsolve(chol((1 - nugget) * V + nugget * diag(n)), diag(n)))
	z <- GLS.fit(formula, data = data, invcholV = invcholV)
	if(verbose == TRUE) show(c(z$logLik, nugget))
	return(z$logLik)
}

nugget.fit <- function(formula, data, V, nugget.tol = 0.00001, interval = c(0, 1), verbose = FALSE) {
	opt.nugget <- optimize(nugget.fit.funct, formula, data = data, V = V, interval = interval, maximum = T, tol = nugget.tol, verbose = verbose)
	# check at the zero boundary
	if(opt.nugget$maximum < nugget.tol){
		nugget0.fit <- nugget.fit.funct(0, formula, data, V)
		if(nugget0.fit > opt.nugget$objective) opt.nugget$maximum <- 0
	}
	return(opt.nugget$maximum)
}
```


### `GLS.partition.data()`

I haven't yet completely wrapped my head arround the partitioned GLS analysis
yet. 

read `RSE MethodsX for the correlated F test` paper from Tony

```{r, eval = FALSE}
############################################################################
# GLS.partition.data fits a GLS to a specified number of random partitions of the data after fitting the nugget. It calls GLS.fit().
############################################################################
GLS.partition.data <- function(formula, formula0 = NULL, data, r, est.nugget = T, npart = 10, partition = NULL, fixed.nugget = NULL, nugget.tol = 0.00001, min.num.rSS = 12) {
	
	max.offdiag.matrices <- ceiling((1 + (1 + 8*min.num.rSS)^.5)/2)
	
	n <- nrow(data) # number of sites
	if (!is.null(partition)) {
		npart <- nrow(partition)
		nn <- n - (n%%npart)
		n.p <- nn/npart
		pick <- partition
	} else {
		nn <- n - (n%%npart) # remove the remainder (if npart doesn't go into n evenly)
		n.p <- nn/npart # sites per partition
		pick <- matrix(sample(n)[1:nn], nrow = npart) # random subsets stacked as rows
	}

	mf <- model.frame(formula = formula, data = data)
	df2 <- n.p - (ncol(model.matrix(attr(mf, "terms"), data = mf)) - 1)
	mf0 <- model.frame(formula = formula0, data = data)
	df0 <- n.p - (ncol(model.matrix(attr(mf0, "terms"), data = mf0)) - 1)
	df1 <- df0 - df2

	SSR.part <- NULL
	SSE.part <- NULL
	SSE0.part <- NULL
	coef.part <- NULL
	coef0.part <- NULL
	se.part <- NULL
	se0.part <- NULL
	F.part <- NULL
	p.F.part <- NULL
	logLik.part <- NULL
	logLik0.part <- NULL
	nugget.part <- NULL
	invcholV.part <- list(NULL)
	xx.part <- list(NULL)
	xx0.part <- list(NULL)
	interval <- c(0,1)
	for (i in 1:npart) {
		
		data.part <- data[pick[i,],] # data partition
		
		# create distance matrix in kilometers
		location <- data.part[,c('lng','lat')]
		Dist.part <- geosphere::distm(location, fun=distGeo)/1000 # partition distance

		Vp <- V.fit(Dist.part, spatialcor = r, FUN = "exponential") # partition variance
		if (is.null(fixed.nugget) & est.nugget) {
			nugget <- nugget.fit(formula, data.part, Vp, interval = interval) # find the nugget
			interval <- c(0, max(1000*nugget.tol, min(100*nugget,1)))  # create interval (after nugget??)
			Vp <- (1 - nugget) * Vp + nugget * diag(n.p) # add nugget to variance
		} else {
			if (is.null(fixed.nugget)) {
				nugget <- 0 # no nugget
				Vp <- Vp
			} else {
				nugget <- fixed.nugget[i]
				Vp <- (1 - nugget) * Vp + nugget * diag(n.p) # add nugget to variance
			}
		}
		z.part <- GLS.fit(formula, formula0, data = data.part, V = Vp) # fit gls for the partition
    # add info to appropriate variables
		SSR.part <- c(SSR.part, z.part$SSR)
		SSE.part <- c(SSE.part, z.part$SSE)
		SSE0.part <- c(SSE0.part, z.part$SSE0)
		coef.part <- cbind(coef.part, z.part$coef)
		coef0.part <- cbind(coef0.part, z.part$coef0)
		se.part <- cbind(se0.part, z.part$se)
		se0.part <- cbind(se.part, z.part$se0)
		F.part <-  c(F.part, z.part$F)
		p.F.part <-  c(p.F.part, z.part$p.F)
		logLik.part <- c(logLik.part, z.part$logLik)
		logLik0.part <- c(logLik0.part, z.part$logLik0)
		nugget.part <- c(nugget.part, nugget)
		if(i <= max.offdiag.matrices){
			invcholV.part[[i]] <- z.part$invcholV
			xx.part[[i]] <- z.part$xx	
			xx0.part[[i]] <- z.part$xx0	
		}
	}
	
	rSSE.part <- matrix(NA, nrow=npart, ncol=npart)
	rSSR.part <- matrix(NA, nrow=npart, ncol=npart)
	for (i in 1:min(max.offdiag.matrices-1,(npart-1))) for (j in (i+1):min(max.offdiag.matrices,npart)) {		
		data.part <- data[c(pick[i,], pick[j,]),] # data from the ith and jth partition together

		# create distance matrix in kilometers
		location <- data.part[,c('lng','lat')]
		Dist.part <- geosphere::distm(location, fun=distGeo)/1000
		Vpick <- V.fit(Dist.part, spatialcor = r, FUN = "exponential") #variance matrix
		Vnugget <- diag(c(rep((1-nugget.part[i])/nugget.part[i], n.p), rep((1-nugget.part[j])/nugget.part[j], n.p))) # nugget matrix
		Vnugget[is.infinite(Vnugget)] <- 0 # no infinite nuggets
		Vpick <- Vpick + Vnugget # add the nugget into the variance matrix
		
		xx1 <- xx.part[[i]]
		xx2 <- xx.part[[j]]
		xx10 <- xx0.part[[i]]
		xx20 <- xx0.part[[j]]
		
		Rij <- crossprod(t(invcholV.part[[i]]), tcrossprod(Vpick[1:n.p, (n.p+1):(2*n.p)], invcholV.part[[j]]))
		H1 <- xx1 %*% solve(t(xx1) %*% xx1) %*% t(xx1) #A %*% (A^T %*% A)^-1 %*% A^T
		H2 <- xx2 %*% solve(t(xx2) %*% xx2) %*% t(xx2)
		
		if(!is.na(xx10[1])){
			H10 <- xx10 %*% solve(t(xx10) %*% xx10) %*% t(xx10)
			H20 <- xx20 %*% solve(t(xx20) %*% xx20) %*% t(xx20)
		}else{
			H10 <- 0
			H20 <- 0
		}
		
		S1R <- H1 - H10
		S2R <- H2 - H20
		
		S1E <- diag(n.p) - H1
		S2E <- diag(n.p) - H2

		rSSR.part[i,j] <- matrix(S1R, nrow=1) %*% matrix(Rij %*% S2R %*% t(Rij), ncol=1)/df1
		rSSE.part[i,j] <- matrix(S1E, nrow=1) %*% matrix(Rij %*% S2E %*% t(Rij), ncol=1)/df2
	}
		
	coef <- rowMeans(coef.part)
	coef0 <- rowMeans(coef0.part)
	rSSR <- mean(rSSR.part, na.rm=T)
	rSSE <- mean(rSSE.part, na.rm=T)
	
	Fmean <- mean(F.part)

	return(list(coef = coef, Fmean = Fmean, df1 = df1, df2 = df2, SSR.part = SSR.part, SSE.part = SSE.part, SSE0.part = SSE0.part, logLik.part = logLik.part, logLik0.part = logLik0.part, nugget = mean(nugget.part), nugget.part = nugget.part, F.part = F.part, p.F.part = p.F.part, coef.part=coef.part, se.part=se.part, coef0.part=coef0.part, se0.part=se0.part, rSSR = rSSR, rSSE = rSSE, rSSR.part = rSSR.part, rSSE.part = rSSE.part, npart = npart, partition = pick))
}
```
























