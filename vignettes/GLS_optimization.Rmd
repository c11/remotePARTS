---
title: "Partition-wise PARTS parameter optimization"
author: "Clay Morrow"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GLSopt}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      error = FALSE)
```

```{r}
library(remotePARTS); library(dplyr)

# make reproducible
set.seed(456)
# should simulation/comparisons be run?
run.sims = FALSE
```

## Introduction

This document will describe how to estimate all spatial parameters simultaneously
from time-series regression coefficients rather than from the time-series residuals.

Let a pixel-level time series model be
$x_i(t) = ... + \beta_i t + \varepsilon_i(t)$ with parameter of interest $\beta$
and an error $\varepsilon(t) \sim N(0, \sigma\Sigma)$ and with spatial autocorrelation
determined by $\Sigma$. PARTS assumes that this spatial autocorrelation matrix
follows the form $\Sigma = ...+ (1 - \eta)f(d, r, [a])$ where $\eta$ is the 
nugget, $f()$ is some function, $d$ is the distance between two pixels, $r$ 
is a spatial range parameter, and $a$ is an optional shape parameter. 

Our goal here is to estimate $\eta$, $r$, and $a$ from a set of estimates 
$\hat{\beta}$.

In this example, we will let $f()$ be an exponential-power covariance function:
$f(d, r, a) = e^{-(\frac{d}{r})^a}$. 

## Example

Here we will run through the steps for this method using the `Alaska` data set.

### Step 1: Estimate $\beta$

The first step in this process is to obtain $\hat{\beta_i}$ for each pixel $i$.
In `remotePARTS`, this can be done using `fitCLS` or `fitAR`, see the documentation
for these functions or the `Alaska` vignette for more information.

### Step 2: Subset the map

Due to computational limits, it will not be possible to use the full dataset to estimate 
spatial parameters for most large maps. For this reason, the next step is to subset
the data into random partitions (see `Alaska` vignette for more information). 
We recommend 20-30 partitions with 2000-3000 pixels each. If your data is smaller,
it is OK to use exhaustive partitioning. `sample_partitions()` makes this sampling
easy. Below we've created a partition matrix for the Alaska dataset:

```{r data_setup, echo = TRUE, eval = FALSE}
## load the dataset
df.file <- system.file("extdata", "AK_ndvi_common-land.csv",
                        package = "remotePARTS")
df <- read.csv(df.file) #read.csv("data-raw/AK_ndvi_common-land.csv")
# map pixels
n.pix = nrow(df)
# 20 partitions
n.part = 20 
# 2000 pixels each
part.size = 2000 
# get partition matrix
parts = sample_partitions(npix = n.pix, partsize = part.size, npart = n.part)
```

### Step 3: Optimze parameters

Next we will use `optimize_GLS()` to estimate $\eta$, $r$, and $a$ simultaneously,
on each partition. Note that estimates will vary among partitons. We've used the
model formula `cls.coef ~ 0 + land` in the GLS, regressing our coefficient
estiamtes `cls.coef` by land class `land`. The default starting parameters
we'll use are r = .1, a = 1, and nugget = 0 (actually, `0` throws an error,
so we need to use `1e-9`). We specify that we want the "exponential-power" 
function and that we want the GLS for each partition saved (`ret.GLS = TRUE`).

```{r, eval = FALSE, echo = TRUE}
# create empty data frame for collection
part.pars = data.frame(NA)

# loop over each partition
for(i in 1:ncol(parts)){
  # subset the data by the indices in the partition:
  df.sub = df[parts[, p], ]
  coords.sub = df.sub[, c("lng", "lat")]
  
  # fit the optimizer, and return the GLS
  opt = optimize_GLS(cls.coef ~ 0 + land, data = df.sub, 
                     pars.start = c(r = .1, a = 1, nug = 1e-9),
                     coords = coords.sub, V.meth = "exponential-power", 
                     ret.GLS = TRUE)
  
  # collect some data
  pars.i <- data.frame(r = opt$spatial.pars["r"],
                         a = opt$spatial.pars["a"],
                         nug = opt$spatial.pars["nug"],
                         B1 = opt$GLS$betahat[1],
                         B2 = opt$GLS$betahat[2],
                         B3 = opt$GLS$betahat[3],
                         p1 = opt$GLS$pval.t[1],
                         p2 = opt$GLS$pval.t[2],
                         p3 = opt$GLS$pval.t[3],
                         partition = p)
  # stack it
  par.pars <- rbind(part.pars, pars.i)
}
```

```{r echo = FALSE}
# load("old-code/spatial-parameter-estimation-comparison_2Kpx.RData",
#      verbose = TRUE)
# load("R/sysdata.rda")
# data(sysdata)
part.pars <- remotePARTS:::pars.iter %>% 
  filter(type == "optim") %>% 
  select(-type, -r.scaled)
```

### Setp 4: Select average estimates

The next step is the most challenging and subjective step. Because the parameter 
estimates may vary (but, will produce similar overall fit), we need to 
select an 'average' or 'representative' set. One approach would be to simply
select the mean or median value from each parameter:

```{r}
summary(part.pars %>% select(r, a, nug))
```

Based on the medians from above table, we could use $r = 0.0385$, 
$a = 0.377$, and $\eta = 0$. 

Or another way would be to find which partition had the most average estimates
and use all parameter values from this partition:

```{r}
part.pars %>% 
  # calculate deviations from the medians
  mutate(r.dev = abs(r - 0.03849), a.dev = abs(a - 0.3765),
         nug.dev = abs(nug - 1.568e-08),
         total.dev = r.dev + a.dev + nug.dev) %>% 
  # arrange by smallest deviation
  arrange(total.dev) %>% 
  select(partition, r, a, nug, total.dev) %>% 
  head(n = 3)
```

From the above table, we can see that the estimates from partion 8 were
the most median (lowest total deviation). So we could use $r = .0438$, $a = 0.377$, 
and $\eta = 0$.

### Step 5: Fit GLS, plugging in spatial parameters

The final step is to then run the GLS, plugging these selected estimates into the
model. See `?fitGLS()`, `?fitGLS.partition()`, and the `Alaska` vignette for 
more information.

for example:

```
fitGLS.partion(..., spatcor = c(r = .438, a = .377), nug = 0, ...)
```

**note** The newest version of `fitGLS.partition` allows for a fixed nugget.

## Comparison with residual method

As mentioned previously, estimating spatial parameters from
the coefficients is much more variable than than when using residuals. Below 
is a figure demonstrating this. The left panel shows semivariograms calculated
from each partition using coefficient estimates (i.e., `optimize_GLS`)
while the right panel shows the semivariograms for the same partitons, 
estimated from the residuals (i.e., `fit_spatialcor()`). Lines are colored
by the value of the estimated spatial range parameter $r$

```{r echo = FALSE, fig.asp = .5, fig.width=5}
library(ggplot2);library(lemon);library(dplyr)

spcor.func = function(d, r, a, nug){
  return((1 - nug)*exp(-(d/r)^a))
}

semivar.table = NULL
for(i in 1:nrow(remotePARTS:::pars.iter)){
  pars = as.data.frame(remotePARTS:::pars.iter[i, ])
  d = seq(0, 1, length.out = 1000)
  r = pars$r
  a = pars$a
  nug = pars$nug
  var = spcor.func(d = d, r = r, a = a, nug = nug)
  semivar = 1-var
  tmp = data.frame(d = d, var = var, semivar = semivar,
                   r = r, a = a, nug = nug,
                   partition = pars$partition, type = pars$type
  )
  semivar.table <- rbind(semivar.table, tmp)
}

ggplot(data = semivar.table, aes(x = d, y = semivar, group = partition)) +
  geom_path(aes(col = r)) +
  geom_point(aes(x = 0, y = nug, col = r), size = 2, shape = "_") +
  facet_rep_wrap(~ type) +
  labs(x = "distance", y = "semivariance") +
  scale_color_viridis_c(option = "plasma", begin = 0, end = 0.75)
```

We can see that, while the residual method maintains the same general covariance
function is found. With the optimized method, the covariance function is much
more variable.

However, **both** methods result in accurate estimates of the GLS coefficients.
The figure below shows this. Each panel represents the GLS coefficient for
each of the 3 land classes. The x-axis represent the individual partitions and
red points are estimates calculated from the optimization method, while blue
points are estimates calculated from the residual method.

```{r, echo = FALSE, fig.asp = .8, fig.width = 5}
# line plot of coefficeint estimates
remotePARTS:::pars.iter %>% reshape2::melt(id.vars = c("partition", "type")) %>%
  filter(variable %in% c("B1", "B2", "B3")) %>%
  ggplot(aes(x = partition, y = value, group = type, 
             shape = type, col = type)) +
  geom_point() +
  geom_line(aes(lty = type)) +
  theme(legend.position = "bottom") +
  labs(lty = NULL, shape = NULL, col = NULL) +
  facet_wrap(~ variable, scales = "free", ncol = 1, strip.position = "left") +
  theme(strip.placement = "outside", strip.background = element_blank()) +
  labs(y = NULL)
```

We can see from this figure that both methods estimate coefficients in a very
similar way for a given set of data. This means that, for most use cases, either
method is viable.


